{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cfd9be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import uuid\n",
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import tensorflow as tf\n",
    "from keras.models import model_from_json\n",
    "from django.http import StreamingHttpResponse\n",
    "from flask import Flask, Response, render_template\n",
    "from flask_socketio import SocketIO, emit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7be6cffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helllo\n"
     ]
    }
   ],
   "source": [
    "signs = {0: \"ayubowan\", 1: \"fifty_2\", 2: \"fifty_3\", 3: \"good(hodai)\", 4: \"his\", 5: \"house\", 6: \"i_love_you\", \n",
    "         7: \"naraka\", 8: \"oba\", 9: \"ten_3\", 10: \"thirty_2\", 11: \"thirty_3\", 12: \"twenty_2\", 13: \"twenty_3\", 14: \"two\", 15: \"when_1\", 16: \"when_2\"}\n",
    "\n",
    "sign_model = open('CNN_model.json', 'r')\n",
    "loaded_json = sign_model.read()\n",
    "sign_model.close()\n",
    "sign_language_model = model_from_json(loaded_json)\n",
    "# weights = np.load('model_weights.npy', allow_pickle=True)\n",
    "# print('Original data shape:', loaded_json)\n",
    "\n",
    "sign_language_model.load_weights(\"CNN_model.h5\")\n",
    "\n",
    "# if weights.shape != (32, 48, 64, 3):\n",
    "# weights = np.reshape(weights, (1, 10, 1, 1))\n",
    "# print('Reshaped data shape:', weights.shape)\n",
    "print(\"helllo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b358b4e3-5196-4df5-8aed-a253062f76c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return '<html><body><img src=\"/video_feed\"></body></html>'\n",
    "\n",
    "def hand_detection():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands:\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # changing from BGR to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, (1280, 720))\n",
    "\n",
    "            # flipping the frame\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            frame.flags.writeable = False\n",
    "\n",
    "            # hand detections\n",
    "            results = hands.process(frame)\n",
    "            frame.flags.writeable = True\n",
    "\n",
    "            # changing from RGB to BGR\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "                    mp_drawing.draw_landmarks(frame, hand, mp_hands.HAND_CONNECTIONS,\n",
    "                                             mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=3, circle_radius=4),\n",
    "                                             mp_drawing.DrawingSpec(color=(221, 44, 250), thickness=2, circle_radius=2),\n",
    "                                             )\n",
    "\n",
    "                    # Get the coordinates of the topmost hand landmark\n",
    "                    x, y = int(hand.landmark[0].x), int(hand.landmark[0].y)\n",
    "\n",
    "                    # Preprocessing the input image\n",
    "                    frame_processed = cv2.resize(frame, (300, 300))\n",
    "                    frame_processed = frame_processed.astype(\"float32\") / 255.0\n",
    "                    frame_processed = np.expand_dims(frame_processed, axis=0)\n",
    "\n",
    "                    prediction = sign_language_model.predict(frame_processed)\n",
    "                    predicted_sign = signs[np.argmax(prediction)]\n",
    "                    maxindex = int(np.argmax(prediction))\n",
    "\n",
    "                    sign_label = signs[maxindex]\n",
    "                    cv2.putText(frame, sign_label, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "            # Convert the image back to 8-bit unsigned integer type (from float type)\n",
    "            frame = (frame * 255).astype(\"uint8\")\n",
    "            ret, buffer = cv2.imencode('.jpg', frame)\n",
    "            frame = buffer.tobytes()\n",
    "            yield (b'--frame\\r\\n'\n",
    "                   b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n",
    "        cap.release()\n",
    "\n",
    "@app.route('/video_feed')\n",
    "def video_feed():\n",
    "    return Response(hand_detection(),\n",
    "                    mimetype='multipart/x-mixed-replace; boundary=frame')\n",
    "\n",
    "app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
